
{% extends "header.html" %}
{% block body %}
<!--       <pre class="prettyprint">              width="750" height="423"    -->
<body class="body">


      <div class="container" align="left" style="max-width:800px">
		<div class="progress">
			<div class="progress-bar" role="progressbar" aria-valuenow="{{completed_percentages['Support Vector Machines (SVM)']}}" aria-valuemin="0" aria-valuemax="100" style="width: {{completed_percentages['Support Vector Machines (SVM)']}}%;">
				Support Vector Machines (SVM) Progress:  {{completed_percentages['Support Vector Machines (SVM)']}}%
			</div>
		</div>
		<img class="img-responsive" src="{{ url_for('static', filename='images/svm/machine-learning-support-vector-machine-linear-svc-example-with-python-and-sklearn-1024x584.png') }}"/>
	  <h2>{{curTitle}}</h2>
	  <br>
	  
	  <div class="embed-responsive embed-responsive-16by9"><iframe width="750" height="423" src="//www.youtube.com/embed/81ZGOib7DTk?list=PLQVvvaa0QuDd0flgGphKCej-9jp-QdzZ3" frameborder="0" allowfullscreen></iframe></div><br>
	  


<p>The most applicable machine learning algorithm for our problem is <a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html" title="Linear SVC machine learning algorithm" target="_blank">Linear SVC</a>. Before hopping into Linear SVC with our data, we're going to show a very simple example that should help solidify your understanding of working with Linear SVC. </p>

<p>The objective of a Linear SVC (Support Vector Classifier) is to fit to the data you provide, returning a "best fit" hyperplane that divides, or categorizes, your data. From there, after getting the hyperplane, you can then feed some features to your classifier to see what the "predicted" class is. This makes this specific algorithm rather suitable for our uses, though you can use this for many situations. Let's get started.</p>

<p>First, we're going to need some basic dependencies:</p>

<pre class="prettyprint">
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style
style.use("ggplot")
from sklearn import svm
</pre>

<p>Matplotlib here is not truly necessary for Linear SVC. The reason why we're using it here is for the eventual data visualization. Typically, you wont be able to visualize as many dimensions as you will have features, but, it's worth visualizing at least once to understand how linear svc works. </p>

<p>Other than the visualization packages we're using, you will just need to import svm from sklearn and numpy for array conversion. </p>

<p>Next, let's consider that we have two features to consider. These features will be visualized as axis on our graph. So something like:</p>

<pre class="prettyprint">
x = [1, 5, 1.5, 8, 1, 9]
y = [2, 8, 1.8, 8, 0.6, 11]
</pre>

<p>Then we can graph this data using:</p>

<pre class="prettyprint">
plt.scatter(x,y)
plt.show()
</pre>

<p>The result is:</p>

<img class="img-responsive" src="{{ url_for('static', filename='images/svm/machine-learning-feature-scatter-plot.png') }}"/>


<p>Now, of course, we can see with our own eyes how these groups should be divided, though exactly where we might draw the dividing line might be debated:</p>

<img class="img-responsive" src="{{ url_for('static', filename='images/svm/machine-learning-division-example.png') }}"/>


<p>So this is with two features, and we see we have a 2D graph. If we had three features, we could have a 3D graph. The 3D graph would be a little more challenging for us to visually group and divide, but still do-able. The problem occurs when we have four features, or four-thousand features. Now you can start to understand the power of machine learning, seeing and analyzing a number of dimensions imperceptible to us.</p>

<p>With that in mind, we're going to go ahead and continue with our two-featured example. Now, in order to feed data into our machine learning algorithm, we first need to compile an array of the features, rather than having them as x and y coordinate values. </p>

<p>Generally, you will see the feature list being stored in a capital X variable. Let's translate our above x and y coordinates into an array that is compiled of the x and y coordinates, where x is a feature and y is a feature.</p>

<pre class="prettyprint">
X = np.array([[1,2],
             [5,8],
             [1.5,1.8],
             [8,8],
             [1,0.6],
             [9,11]])
</pre>

<p>Now that we have this array, we need to label it for training purposes. There are forms of machine learning called "unsupervised learning," where data labeling isn't used, as is the case with clustering, though this example is a form of supervised learning. </p>

<p>For our labels, sometimes referred to as "targets," we're going to use 0 or 1. </p>

<pre class="prettyprint">
y = [0,1,0,1,0,1]
</pre>

<p>Just by looking at our data set, we can see we have coordinate pairs that are "low" numbers and coordinate pairs that are "higher" numbers. We've then assigned 0 to the lower coordinate pairs and 1 to the higher feature pairs. </p>

<p>These are the labels. In the case of our project, we will wind up having a list of numerical features that are various statistics about stock companies, and then the "label" will be either a 0 or a 1, where 0 is under-perform the market and a 1 is out-perform the market. </p>

<p>Moving along, we are now going to define our classifier:</p>

<pre class="prettyprint">
clf = svm.SVC(kernel='linear', C = 1.0)
</pre>

<p>We're going to be using the SVC (support vector classifier) SVM (support vector machine). Our kernel is going to be linear, and C is equal to 1.0. What is C you ask? Don't worry about it for now, but, if you must know, C is a valuation of "how badly" you want to properly classify, or fit, everything. The machine learning field is relatively new, and experimental. There exist many debates about the value of C, as well as how to calculate the value for C. We're going to just stick with 1.0 for now, which is a nice default parameter. </p>

<p>Next, we call:</p>

<pre class="prettyprint">
clf.fit(X,y)
</pre>

<p>From here, the learning is done. It should be nearly-instant, since we have such a small data set. </p>

<p>Next, we can predict and test. Let's print a prediction:</p>

<pre class="prettyprint">
print(clf.predict([0.58,0.76]))
</pre>

<p>We're hoping this predicts a 0, since this is a "lower" coordinate pair. </p>

<p>Sure enough, the prediction is a classification of 0. Next, what if we do:</p>

<pre class="prettyprint">
print(clf.predict([10.58,10.76]))
</pre>

<p>And again, we have a theoretically correct answer of 1 as the classification. This was a blind prediction, though it was really a test as well, since we knew what the hopeful target was. Congratulations, you have 100% accuracy!</p>

<p>Now, to visualize your data:</p>

<pre class="prettyprint">
w = clf.coef_[0]
print(w)

a = -w[0] / w[1]

xx = np.linspace(0,12)
yy = a * xx - clf.intercept_[0] / w[1]

h0 = plt.plot(xx, yy, 'k-', label="non weighted div")

plt.scatter(X[:, 0], X[:, 1], c = y)
plt.legend()
plt.show()
</pre>

<p>The result:</p>

<img class="img-responsive" src="{{ url_for('static', filename='images/svm/machine-learning-support-vector-machine-linear-svc-example-with-python-and-sklearn-1024x584.png') }}"/>

<p>If you'd like a bit more explanation on how the graphing code works, watch the second-half of the embedded video. Visualizing the data is somewhat useful to see what the program is doing in the background, but is not really necessary to understand how to visualize it specifically at this point. You will likely find that the problems you are trying to solve simply cannot be visualized due to having too many features and thus too many dimensions to graph.</p>
	  
		
		
		
		<p>The next tutorial: <a title="{{nextTitle}}" href="{{nextLink}}?completed={{curLink}}"><button class="btn btn-primary">{{nextTitle}}</button></a></p>
	  </div>


	

	
</body>


{% endblock %}


